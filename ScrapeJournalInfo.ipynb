{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "issue finished!\n",
      "end all available issues!\n"
     ]
    }
   ],
   "source": [
    "# coding:utf-8\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import certifi\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import chromedriver_binary\n",
    "import csv\n",
    "\n",
    "# アクセスするURL\n",
    "vol_num = 15\n",
    "issue_num = 1\n",
    "while vol_num < 29:\n",
    "    root_url = \"https://www.sciencedirect.com\"\n",
    "    url = root_url + \"/journal/cirp-journal-of-manufacturing-science-and-technology/vol/\"  + str(vol_num) \n",
    "\n",
    "    # 書き込み用CSV\n",
    "    csv_file = \"C:\\\\Users\\<username>\\Documents\\JournalList\\journal_list_vol\" + str(vol_num) + \".csv\"\n",
    "\n",
    "    # ブラウザーを起動\n",
    "    options = Options()\n",
    "\n",
    "    options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(r\"C:\\Users\\saori\\Anaconda3\\Lib\\site-packages\\chromedriver_binary\\chromedriver.exe\")\n",
    "\n",
    "    # httpsの証明書検証を実行している\n",
    "    http = urllib3.PoolManager(\n",
    "        cert_reqs='CERT_REQUIRED',\n",
    "        ca_certs=certifi.where())\n",
    "    r = http.request('GET', url)\n",
    "\n",
    "    soup = BeautifulSoup(r.data, 'html.parser')\n",
    "\n",
    "    # 初期化\n",
    "    header_data = []\n",
    "    header_author_data = []\n",
    "    author_list_append = []\n",
    "\n",
    "    # 論文リストから要素を抽出\n",
    "    list_all = soup.find_all(\"li\", class_=\"js-article-list-item article-item u-padding-xs-top u-margin-l-bottom\")\n",
    "\n",
    "    #各論文ページを参照\n",
    "    for list_row in list_all :\n",
    "\n",
    "        # 初期化\n",
    "        inst_searchlist = []\n",
    "\n",
    "        additional_url = list_row.find(\"a\", attrs={\"class\", \"anchor article-content-title u-margin-xs-top u-margin-s-bottom\"}).get('href')\n",
    "        url_child = root_url + additional_url\n",
    "\n",
    "\n",
    "        #Beautiful Soup\n",
    "        r_child = http.request('GET', url_child)\n",
    "        child_data = BeautifulSoup(r_child.data, 'html.parser')\n",
    "\n",
    "        title = child_data.find(\"span\", attrs={\"class\", \"title-text\"}).string\n",
    "\n",
    "\n",
    "        #Web Driver\n",
    "        driver.get(url_child)\n",
    "\n",
    "        #Show Detailsを開ける\n",
    "        sleep(3)\n",
    "        driver.find_element_by_class_name(\"show-hide-details.u-font-sans\").click()\n",
    "\n",
    "        #Show Detailsを開けた後の状態で取得しなおす\n",
    "        sleep(1)\n",
    "\n",
    "        inst_list = driver.find_elements_by_class_name(\"affiliation\")    \n",
    "        author_list = child_data.find_all(\"a\", attrs={\"class\", \"author size-m workspace-trigger\"})\n",
    "\n",
    "        # 所属機関取得\n",
    "        inst_list_len = len(inst_list)\n",
    "\n",
    "        if inst_list_len > 1:\n",
    "\n",
    "            for an_inst in inst_list :     \n",
    "\n",
    "                # 複数人著者がいるのに、一人ごとに所属機関が記載されている場合の対策                 \n",
    "                try: \n",
    "                    an_inst.find_element_by_tag_name(\"sup\")\n",
    "                except:\n",
    "                    inst_ref = None\n",
    "                else:\n",
    "                    inst_ref = an_inst.find_element_by_tag_name(\"sup\").text\n",
    "                    \n",
    "                inst_name = an_inst.find_element_by_tag_name(\"dd\").text\n",
    "\n",
    "                # 所属機関が複数ある場合のみinst_listを使う\n",
    "                inst_searchlist.append([inst_ref, inst_name])\n",
    "\n",
    "        elif inst_list_len == 1:\n",
    "            for an_inst in inst_list :    \n",
    "                inst_ref = None            \n",
    "                inst_name = an_inst.find_element_by_tag_name(\"dd\").text\n",
    "\n",
    "        else:\n",
    "            inst_ref = None\n",
    "            inst_name = None\n",
    "\n",
    "\n",
    "\n",
    "        # 著者リスト取得\n",
    "        for an_author in author_list :\n",
    "\n",
    "            given_name = an_author.find(\"span\", attrs={\"class\", \"text given-name\"}).string\n",
    "            surname = an_author.find(\"span\", attrs={\"class\", \"text surname\"}).string\n",
    "            name = given_name + surname\n",
    "\n",
    "            author_ref = an_author.find(\"span\", attrs={\"class\", \"author-ref\"})\n",
    "\n",
    "            # 著者名の右にレファレンス番号（aなど）があった場合、所属機関検索リストから検索\n",
    "            if author_ref is not None:\n",
    "                author_ref = author_ref.string\n",
    "                for each in inst_searchlist:\n",
    "                    if author_ref == each[0]:\n",
    "                        inst_name = each[1]\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "            author_list_append.append([vol_num, issue_num, title, url_child, name, author_ref, inst_name])\n",
    "\n",
    "\n",
    "    with open(csv_file, 'w', encoding='UTF-8', newline=\"\") as f:\n",
    "        writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "        writer.writerows(author_list_append)\n",
    "\n",
    "    vol_num = vol_num + 1     \n",
    "    \n",
    "#     if issue_num == 4:\n",
    "#         vol_num = vol_num + 1\n",
    "#         issue_num = 1\n",
    "#     else:\n",
    "#         issue_num = issue_num + 1\n",
    "    \n",
    "    print(\"issue finished!\")\n",
    "print(\"end all available issues!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
